{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f65255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled list of training files:\n",
    "train_files = pd.read_csv('data/task2/train/labels.csv', index_col=0)\n",
    "train_files['file'] = ['data/task2/train/' + s for s in train_files['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bb37a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data sample:\n",
    "file_path = train_files.file.sample(1).iloc[0]\n",
    "for i in range(10000):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"UnicodeDecodeError in file: {file_path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dec5c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "file",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "da574a5a-a0f7-40dd-aeca-c112a0debe7a",
       "rows": [
        [
         "0",
         "data/task2/test/0000.txt"
        ],
        [
         "1",
         "data/task2/test/0001.txt"
        ],
        [
         "2",
         "data/task2/test/0002.txt"
        ],
        [
         "3",
         "data/task2/test/0003.txt"
        ],
        [
         "4",
         "data/task2/test/0004.txt"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/task2/test/0000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/task2/test/0001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/task2/test/0002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/task2/test/0003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/task2/test/0004.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file\n",
       "0  data/task2/test/0000.txt\n",
       "1  data/task2/test/0001.txt\n",
       "2  data/task2/test/0002.txt\n",
       "3  data/task2/test/0003.txt\n",
       "4  data/task2/test/0004.txt"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load list of test files:\n",
    "import os\n",
    "test_files = ['data/task2/test/' + s for s in os.listdir('data/task2/test/')]\n",
    "test_files.sort()\n",
    "test_files = pd.DataFrame({'file': test_files})\n",
    "test_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30816d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: texts, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels})\n\u001b[0;32m     10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m load_data(train_files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m], train_files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 8\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_list, labels)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:940\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[1;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[0;32m    938\u001b[0m     arrow_typed_mapping[col] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    939\u001b[0m mapping \u001b[38;5;241m=\u001b[39m arrow_typed_mapping\n\u001b[1;32m--> 940\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\table.py:758\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pydict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pydict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\table.pxi:1982\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\table.pxi:6379\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:405\u001b[0m, in \u001b[0;36mpyarrow.lib.asarray\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:255\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\array.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_writer.py:220\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# automatic type inference for custom objects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtry_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_custom_type_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtry_type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_type \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_writer.py:192\u001b[0m, in \u001b[0;36mTypedSequence._infer_custom_type_and_encode\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mPIL_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPIL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     non_null_idx, non_null_value \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_non_null_non_empty_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(non_null_value, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [Image()\u001b[38;5;241m.\u001b[39mencode_example(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m data], Image()\n",
      "File \u001b[1;32mc:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:326\u001b[0m, in \u001b[0;36mfirst_non_null_non_empty_value\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst_non_null_non_empty_value\u001b[39m(iterable):\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the index and the value of the first non-null non-empty value in the iterable. If all values are None or empty, return -1 as index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    328\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m i, value\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def load_data(file_list, labels=None):\n",
    "    texts = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "train_data = load_data(train_files['file'], train_files['label'])\n",
    "\n",
    "test_data = load_data(test_files['file'])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding = \"max_length\", truncation = True, max_length = 512)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "train_data.set_format(type = 'torch', columns = ['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type = 'torch', columns = ['input_ids', 'attention_mask'])\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(p.predictions, axis = 1)\n",
    "    return {'f1': f1_score(p.label_ids, preds)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = 3,              \n",
    "    per_device_train_batch_size = 16,  \n",
    "    per_device_eval_batch_size = 64,   \n",
    "    warmup_steps = 500,                \n",
    "    weight_decay = 0.01,               \n",
    "    logging_dir = './logs',            \n",
    "    logging_steps = 10,\n",
    "    evaluation_strategy = \"epoch\",     \n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'f1',      \n",
    "    greater_is_better = True,          \n",
    "    learning_rate = 5e-6              \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_data,           \n",
    "    eval_dataset = test_data,              \n",
    "    compute_metrics = compute_metrics      \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "pred_labels = torch.argmax(predictions.predictions, axis = 1).cpu().numpy()\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns = ['predictions']).to_csv('submission.csv')\n",
    "\n",
    "submission.to_csv('submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
