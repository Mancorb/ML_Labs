{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f65255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\drago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d0f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled list of training files:\n",
    "train_files = pd.read_csv('data/task2/train/labels.csv', index_col=0)\n",
    "train_files['file'] = ['data/task2/train/' + s for s in train_files['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bb37a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data sample:\n",
    "file_path = train_files.file.sample(1).iloc[0]\n",
    "for i in range(10000):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"UnicodeDecodeError in file: {file_path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of test files:\n",
    "import os\n",
    "test_files = ['data/task2/test/' + s for s in os.listdir('data/task2/test/')]\n",
    "test_files.sort()\n",
    "test_files = pd.DataFrame({'file': test_files})\n",
    "test_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30816d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def load_data(file_list, labels=None):\n",
    "    texts = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            texts.append(f.read())\n",
    "    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "train_data = load_data(train_files['file'], train_files['label'])\n",
    "\n",
    "test_data = load_data(test_files['file'])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding = \"max_length\", truncation = True, max_length = 512)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "train_data.set_format(type = 'torch', columns = ['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type = 'torch', columns = ['input_ids', 'attention_mask'])\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(p.predictions, axis = 1)\n",
    "    return {'f1': f1_score(p.label_ids, preds)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = 3,              \n",
    "    per_device_train_batch_size = 16,  \n",
    "    per_device_eval_batch_size = 64,   \n",
    "    warmup_steps = 500,                \n",
    "    weight_decay = 0.01,               \n",
    "    logging_dir = './logs',            \n",
    "    logging_steps = 10,\n",
    "    evaluation_strategy = \"epoch\",     \n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'f1',      \n",
    "    greater_is_better = True,          \n",
    "    learning_rate = 5e-6              \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_data,           \n",
    "    eval_dataset = test_data,              \n",
    "    compute_metrics = compute_metrics      \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "pred_labels = torch.argmax(predictions.predictions, axis = 1).cpu().numpy()\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns = ['predictions']).to_csv('submission.csv')\n",
    "\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "files.download('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
